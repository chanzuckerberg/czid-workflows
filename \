import os
import re
import time
import random
from typing import Dict
import boto3
import json
import requests
import logging
from botocore.exceptions import ClientError

log = logging.getLogger(__name__)

MAX_CHUNKS_IN_FLIGHT = 10


batch_client = boto3.client("batch")


def get_batch_job_desc_bucket():
    try:
        account_id = boto3.client("sts").get_caller_identity()["Account"]
    except ClientError:
        account_id = requests.get(
            "http://169.254.169.254/latest/dynamic/instance-identity/document"
        ).json()["accountId"]
    return f"aegea-batch-jobs-{account_id}"


class BatchJobFailed(Exception):
    pass


def _get_job_status(job_id):
    batch_job_desc_bucket = boto3.resource("s3").Bucket(get_batch_job_desc_bucket())
    key = f"job_descriptions/{job_id}"
    try:
        job_desc_object = batch_job_desc_bucket.Object(key)
        return json.loads(job_desc_object.get()["Body"].read())["status"]
    except ClientError as e:
        if e.response["Error"]["Code"] == "NoSuchKey":
            # Warn that the object is missing so any issue with the s3 mechanism can be identified
            log.debug("missing_job_description_ojbect", extra={key: key})
            # Return submitted because a missing job status probably means it hasn't been added yet
            return "SUBMITTED"
        else:
            raise e

def _run_batch_job(
    job_name: str,
    job_queue: str,
    job_definition: str,
    environment: Dict[str, str],
    retries: int,
):
    def _log_status(status: str):
        log.info(
            "batch_job_status",
            extra={
                "job_id": job_id,
                "job_name": job_name,
                "job_queue": job_queue,
                "job_definition": job_definition,
                "status": status,
            },
        )

    response = batch_client.submit_job(
        jobName=job_name,
        jobQueue=job_queue,
        jobDefinition=job_definition,
        containerOverrides={
            "environment": environment,
        },
        retryStrategy={"attempts": retries},
    )
    job_id = response["jobId"]
    _log_alignment_batch_job_status(
        job_id, job_name, job_queue, job_definition, "SUBMITTED",
    )

    delay = 60 + random.randint(
        -60 // 2, 60 // 2
    )  # Add some noise to de-synchronize chunks
    status = "SUBMITTED"
    # the job this is monitoring has an timeout and the job this runs in has a timeout
    while True:
        try:
            status = _get_job_status(job_id)
        except ClientError as e:
            # If we get throttled, randomly wait to de-synchronize the requests
            if e.response["Error"]["Code"] == "TooManyRequestsException":
                log.warn("describe_jobs_rate_limit_error", extra={"job_id": job_id})
                # Possibly implement a backoff here if throttling becomes an issue
            else:
                log.error(
                    "unexpected_client_error_while_polling_job_status",
                    extra={"job_id": job_id},
                )
                raise e

        if status == "SUCCEEDED":
            _log_alignment_batch_job_status(
                job_id, job_queue, job_definition, chunk_id, status, alignment_algorithm
            )
            return job_id
        if status == "FAILED":
            log.error(
                "alignment_batch_job_failed",
                extra={
                    "job_id": job_id,
                    "chunk_id": chunk_id,
                    "alignment_algorithm": alignment_algorithm,
                },
            )
            _log_alignment_batch_job_status(
                job_id, job_queue, job_definition, chunk_id, status, alignment_algorithm
            )
            raise BatchJobFailed("chunk alignment failed")
        time.sleep(delay)


def run_chunk(alignment_algorithm: str, provisioning_model: str, chunk_id: int, input_dir: str):
    deployment_environment = os.environ["DEPLOYMENT_ENVIRONMENT"]
    pattern = r"s3://.+/samples/([0-9]+)/([0-9]+)/"
    m = re.match(pattern, input_dir)
    if m:
        project_id, sample_id = m.group(1), m.group(2)
    else:
        project_id, sample_id = "0", "0"

    priority_name = os.environ.get("PRIORITY_NAME", "normal")
    job_name = (f"idseq-{deployment_environment}-{alignment_algorithm}-"
                f"project-{project_id}-sample-{sample_id}-part-{chunk_id}")
    job_queue = f"idseq-{deployment_environment}-{alignment_algorithm}-{provisioning_model}-{priority_name}"
    job_definition = f"" # TODO

    environment = {} # TODO
    retries = 2 if provisioning_model == "SPOT" else 1

def db_chunks(bucket: str, prefix):
    s3_client = boto3.client("s3")
    paginator = s3_client.get_paginator("list_objects_v2")
    log.debug("db chunks")

    result = paginator.paginate(
        Bucket=bucket,
        Prefix=prefix,
    )

    for page in result:
        for obj in page["Contents"]:
            yield obj["Key"]
