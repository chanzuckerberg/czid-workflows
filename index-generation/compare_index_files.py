#!/usr/bin/env python3

import os

from sys import argv

from pprint import pprint  # noqa
from typing import Any, List, Optional, Tuple, Union

import boto3  # type: ignore


s3 = boto3.client("s3", endpoint_url=os.environ.get('ENDPOINT_URL'))

BUCKET = os.environ['BUCKET']  # NOTE: this can be overridden
OUTPUT_DIRS = (
    'taxonomy',  # lineages go here
    'alignment_data',  # accessions go here
    'alignment_indexes',  # gsnap and rapsearch2 indexes go here
)
BLACKLIST = (
    '.log',
    '.md5',
    'status.json',
    '/',  # for some reason dir keys are being returned
    '.sqlite3',  # sqlite3 is deprecated
    '.sqlite3.lz4',
    'README',
    '_files_stats.txt',  # generated by this script
)
MISSING = '!!!MISSING!!!'


def compare_all(ds1: str, ds2: Optional[str] = None) -> None:
    """
    Compares all sets of IDseq index files from two dates. If second arg is
    omitted, the most recent previous date will be used.
    >>> compare_all('2020-02-10')
    -== Comparing contents of s3://czid-public-references/taxonomy/ ==-
    <BLANKLINE>
    FILENAME                                      2020-02-10      2020-02-03          % DIFF
    deuterostome_taxids.txt                           749 KB          748 KB            0.1%
    ...
    taxid-lineages.db.lz4                              38 MB           53 MB          -28.2%
    <BLANKLINE>
    <BLANKLINE>
    -== Comparing contents of s3://czid-public-references/alignment_data/ ==-
    <BLANKLINE>
    FILENAME                                      2020-02-10      2020-02-03          % DIFF
    accession2taxid.db                                 64 GB           64 GB            0.0%
    ...
    taxid2wgs_accession.db.lz4                          2 MB            2 MB            2.3%
    <BLANKLINE>
    <BLANKLINE>
    -== Comparing contents of s3://czid-public-references/alignment_indexes/ ==-
    <BLANKLINE>
    FILENAME                                      2020-02-10      2020-02-03          % DIFF
    nr.gz                                              67 GB           52 GB           26.9%
    ...
    nt_k16.version                                       7 B             7 B            0.0%
    <BLANKLINE>
    <BLANKLINE>
    >>> compare_all('2019-01-21')
    Traceback (most recent call last):
    ...
    AssertionError: Previous dirs must all be named the same: ['2018-12-05', '2018-12-01_asgtest', '2018-12-05']
    """
    if ds2 is None:
        all_ds2 = [_get_prev_date(ds1, output_dir) for output_dir in OUTPUT_DIRS]  # pyre-ignore
        assert all(
            ds2 == all_ds2[0] for ds2 in all_ds2), 'Previous dirs must all be named the same: {}'.format(all_ds2)
        ds2 = all_ds2[0]

    for output_dir in OUTPUT_DIRS:
        url = 's3://{}/{}/'.format(BUCKET, output_dir)  # pyre-ignore
        print('-== Comparing contents of {} ==-\n'.format(url))
        compare(ds1, ds2, output_dir)
        print('\n')


def compare(ds1: str, ds2: str, output_dir: str) -> None:
    """
    Compares sets of IDseqs index files from two dates in a given output dir.
    >>> compare('2020-02-10', '2020-02-03', 'taxonomy')
    FILENAME                                      2020-02-10      2020-02-03          % DIFF
    deuterostome_taxids.txt                           749 KB          748 KB            0.1%
    ...
    respiratory_taxon_whitelist.txt                    348 B   !!!MISSING!!!   !!!MISSING!!!
    ...
    >>> compare('2020-02-10', '2020-02-03', 'asdf')
    Traceback (most recent call last):
    ...
    ValueError: Unknown dir asdf
    >>> compare('2020-02-10', '2020-99-99', 'taxonomy')
    FILENAME                                      2020-02-10      2020-99-99          % DIFF
    deuterostome_taxids.txt                           749 KB   !!!MISSING!!!   !!!MISSING!!!
    ...
    >>> compare('2020-99-99', '2020-02-03', 'taxonomy')
    FILENAME                                      2020-99-99      2020-02-03          % DIFF
    deuterostome_taxids.txt                    !!!MISSING!!!          748 KB   !!!MISSING!!!
    ...
    """
    if output_dir not in OUTPUT_DIRS:
        raise ValueError('Unknown dir {}'.format(output_dir))

    ds1_files = _get_file_metadata(ds1, output_dir)
    ds2_files = _get_file_metadata(ds2, output_dir)
    rows = _get_rows(ds1_files, ds2_files)
    headers = ['FILENAME', ds1, ds2, r'% DIFF']
    # return [headers] + rows
    _print_table([headers] + rows)


def _print_table(table: List[List[Any]]):
    """
    >> _print_table([['a', 'b', 'c', 'd'], ['a', 'b', 'c', 'd']])
    a                    b               c               d
    a                    b               c               d
    """
    for row in table:
        print('{:<40} {:>15} {:>15} {:>15}'.format(*row))


def _get_rows(ds1_files: List[Tuple], ds2_files: List[Tuple]) -> List[List]:
    """
    >>> pprint(_get_rows([('taxonomy/2020-02-10/deuterostome_taxids.txt', 766366),
    ... ('taxonomy/2020-02-10/MISSING.txt', 766366)], [('taxonomy/2020-02-03/deuterostome_taxids.txt', 666366)]))
    [['MISSING.txt', '748 KB', '!!!MISSING!!!', '!!!MISSING!!!'],
     ['deuterostome_taxids.txt', '748 KB', '651 KB', '15.0%']]
    """
    ds1_dict = dict((f[0].split('/')[-1], f) for f in ds1_files)
    ds2_dict = dict((f[0].split('/')[-1], f) for f in ds2_files)
    all_filenames = set(ds1_dict.keys()) | set(ds2_dict.keys())

    rows = []
    for filename in sorted(all_filenames):
        if filename in ds1_dict:
            ds1_size = ds1_dict[filename][1]
        else:
            ds1_size = MISSING  # pyre-ignore

        if filename in ds2_dict:
            ds2_size = ds2_dict[filename][1]
        else:
            ds2_size = MISSING

        if ds1_size == MISSING or ds2_size == MISSING:
            percent = MISSING
        else:
            percent = '{0:.1%}'.format((ds1_size - ds2_size) / ds2_size)

        rows.append([
            filename,
            ds1_size if ds1_size == MISSING else _get_readable_file_size(ds1_size),
            ds2_size if ds2_size == MISSING else _get_readable_file_size(ds2_size),
            percent
        ])

    return rows


def _get_prev_date(ds: str, output_dir: str) -> str:
    """
    >>> _get_prev_date('2020-02-10', 'taxonomy')
    '2020-02-03'
    >>> _get_prev_date('1920-02-10', 'taxonomy')
    Traceback (most recent call last):
    ...
    ValueError: 1920-02-10 dir not found
    """
    files = _get_file_metadata('', output_dir)
    dirs = sorted(set([f[0].split('/')[1] for f in files]))
    for i, dirname in enumerate(dirs):
        if dirname == ds:
            return dirs[i - 1]
    raise ValueError('{} dir not found'.format(ds))


def _get_file_metadata(ds: str, output_dir: str) -> List[Tuple]:
    """
    >>> files = _get_file_metadata('2020-02-03', 'taxonomy')
    >>> files[0]
    ('taxonomy/2020-02-03/deuterostome_taxids.txt', 766366)
    >>> files[-1]
    ('taxonomy/2020-02-03/taxid-lineages.db.lz4', 55411789)
    >>> len(files)
    6
    """
    prefix = '{}/{}'.format(output_dir, ds)
    s3_list = s3.list_objects_v2(Bucket=BUCKET, Prefix=prefix, MaxKeys=1000)  # pyre-ignore
    if 'Contents' in s3_list:
        # See https://docs.aws.amazon.com/AmazonS3/latest/API/API_Object.html
        return sorted([
            (obj['Key'], obj['Size'])
            for obj in s3_list['Contents']
            if not obj['Key'].endswith(BLACKLIST)  # pyre-ignore
        ])
    else:
        return []


def _get_readable_file_size(size_in_bytes: Union[str, int]) -> str:
    """
    Adapted from https://stackoverflow.com/a/58201995 .
    >>> _get_readable_file_size(85920768)
    '82 MB'
    """
    if isinstance(size_in_bytes, str):
        return size_in_bytes

    assert isinstance(size_in_bytes, int)

    size_units = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']
    index = 0
    readable_size = float(size_in_bytes)
    while readable_size >= 1024:
        readable_size /= 1024
        index += 1
    try:
        return '{} {}'.format(round(readable_size), size_units[index])
    except IndexError:
        return 'File too large'


def _from_s3_address(s3_address: str) -> Tuple[str, str, str]:
    """
    >>> _from_s3_address('s3://czid-public-references/alignment_data/2020-02-03/')
    ('czid-public-references', 'alignment_data', '2020-02-03')
    """
    parts = s3_address.strip('/').split('/')
    return parts[2], parts[-2], parts[-1]


if __name__ == '__main__':
    # import doctest
    # doctest.testmod(optionflags=doctest.FAIL_FAST | doctest.ELLIPSIS)

    if argv[1].startswith('s3://'):
        BUCKET, output_dir, ds1 = _from_s3_address(argv[1])
        ds2 = _get_prev_date(ds1, output_dir)
        compare(ds1, ds2, output_dir)
    else:
        compare_all(argv[1], argv[2] if len(argv) == 3 else None)
